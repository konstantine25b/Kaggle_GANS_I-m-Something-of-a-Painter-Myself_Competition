{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"},{"sourceId":14174380,"sourceType":"datasetVersion","datasetId":9035128}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"dc82bfba","cell_type":"code","source":"\nimport os\nimport shutil\nimport zipfile\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport glob\n\n# --- CONFIGURATION ---\n# Kaggle Paths\nMONET_PATH = '../input/gan-getting-started/monet_jpg'\nPHOTO_PATH = '../input/gan-getting-started/photo_jpg'\n\n# --- DYNAMIC WEIGHTS SEARCH ---\n# We search for ANY .pth file in ../input/\n# This handles cases where the dataset name is different or files are nested.\nprint(\"Searching for weights in ../input/ ...\")\nfound_weights = glob.glob('../input/**/*.pth', recursive=True)\n\nWEIGHTS_PATH = None\n\n# Prioritize files with 'Monet' and 'epoch' in the name if multiple exist\nfor f in found_weights:\n    if 'G_Monet' in f and 'epoch' in f:\n        WEIGHTS_PATH = f\n        break\n\n# Fallback: take the first .pth found if no specific match\nif WEIGHTS_PATH is None and len(found_weights) > 0:\n    WEIGHTS_PATH = found_weights[0]\n\nif WEIGHTS_PATH:\n    print(f\"✅ Found weights at: {WEIGHTS_PATH}\")\nelse:\n    print(\"❌ No weights found! Please check your dataset.\")\n    # Default fallback for local testing only\n    WEIGHTS_PATH = 'G_Monet_epoch_30_2.pth'\n\n# Output\nOUTPUT_DIR = '../tmp/images'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T09:43:52.977177Z","iopub.execute_input":"2025-12-16T09:43:52.977502Z","iopub.status.idle":"2025-12-16T09:43:56.423771Z","shell.execute_reply.started":"2025-12-16T09:43:52.977477Z","shell.execute_reply":"2025-12-16T09:43:56.422396Z"}},"outputs":[{"name":"stdout","text":"Searching for weights in ../input/ ...\n✅ Found weights at: ../input/monet-gan-weights/G_Photo_epoch_30_2.pth\nUsing device: cpu\n","output_type":"stream"}],"execution_count":19},{"id":"1b3b1afb","cell_type":"code","source":"\n# --- MODELS (Inlined from src/models/generator/resnet_gan.py) ---\n\nclass ResNetBlock(nn.Module):\n    def __init__(self, dim):\n        super(ResNetBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(dim, dim, kernel_size=3, padding=0),\n            nn.InstanceNorm2d(dim),\n            nn.ReLU(True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(dim, dim, kernel_size=3, padding=0),\n            nn.InstanceNorm2d(dim)\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\nclass ResNetGenerator(nn.Module):\n    def __init__(self, input_nc=3, output_nc=3, ngf=32, n_blocks=6, n_downsampling=2):\n        super(ResNetGenerator, self).__init__()\n        \n        # Initial convolution\n        model = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n            nn.InstanceNorm2d(ngf),\n            nn.ReLU(True)\n        ]\n\n        # Downsampling\n        in_features = ngf\n        out_features = in_features * 2\n        for _ in range(n_downsampling):\n            model += [\n                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(True)\n            ]\n            in_features = out_features\n            out_features = in_features * 2\n\n        # Residual blocks\n        for _ in range(n_blocks):\n            model += [ResNetBlock(in_features)]\n\n        # Upsampling\n        out_features = in_features // 2\n        for _ in range(n_downsampling):\n            model += [\n                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(True)\n            ]\n            in_features = out_features\n            out_features = in_features // 2\n\n        # Output layer\n        model += [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n            nn.Tanh()\n        ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T09:44:02.138767Z","iopub.execute_input":"2025-12-16T09:44:02.139033Z","iopub.status.idle":"2025-12-16T09:44:02.149795Z","shell.execute_reply.started":"2025-12-16T09:44:02.139013Z","shell.execute_reply":"2025-12-16T09:44:02.148811Z"}},"outputs":[],"execution_count":20},{"id":"fe744d95","cell_type":"code","source":"\n# --- DATASET & TRANSFORMS ---\n\nclass ImageDataset(Dataset):\n    def __init__(self, files, transform=None):\n        self.files = files\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img_path = self.files[index]\n        img = Image.open(img_path).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, os.path.basename(img_path)\n\n    def __len__(self):\n        return len(self.files)\n\ndef get_transforms():\n    # Only resize and normalize for inference\n    return transforms.Compose([\n        transforms.Resize((256, 256), Image.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\ndef denormalize(tensor):\n    return tensor * 0.5 + 0.5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T09:44:11.451423Z","iopub.execute_input":"2025-12-16T09:44:11.451723Z","iopub.status.idle":"2025-12-16T09:44:11.458960Z","shell.execute_reply.started":"2025-12-16T09:44:11.451705Z","shell.execute_reply":"2025-12-16T09:44:11.457791Z"}},"outputs":[],"execution_count":23},{"id":"3232f68a","cell_type":"code","source":"\n# --- MAIN EXECUTION ---\n\n# 1. Initialize Model\n# Experiment 2 used defaults: ngf=32, n_blocks=6\nmodel = ResNetGenerator(ngf=32, n_blocks=6).to(DEVICE)\n\n# 2. Load Weights\n# Fallback check for weights in current directory if dataset path fails\nif not os.path.exists(WEIGHTS_PATH):\n    if os.path.exists('G_Monet_epoch_30_2.pth'):\n        WEIGHTS_PATH = 'G_Monet_epoch_30_2.pth'\n        print(f\"Found weights in current directory: {WEIGHTS_PATH}\")\n    else:\n        print(f\"⚠️ Weights not found at {WEIGHTS_PATH}. Please make sure you added the dataset correctly.\")\n\nif os.path.exists(WEIGHTS_PATH):\n    print(f\"Loading weights from {WEIGHTS_PATH}...\")\n    state_dict = torch.load(WEIGHTS_PATH, map_location=DEVICE)\n    model.load_state_dict(state_dict)\nelse:\n    raise FileNotFoundError(f\"Could not find weight file at {WEIGHTS_PATH}\")\n\n\nprint(f\"Model initialized with ngf=32, n_blocks=6\")\n# print(model) # Uncomment to debug structure\n\nmodel.eval()\n\n# 3. Prepare Data\nimport glob\nphoto_files = sorted(glob.glob(os.path.join(PHOTO_PATH, \"*.jpg\")))\nprint(f\"Found {len(photo_files)} photo images.\")\n\n# 4. Generate Images\ndataset = ImageDataset(photo_files, transform=get_transforms())\nloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n\nprint(\"Starting generation...\")\ncnt = 0\n\nwith torch.no_grad():\n    for img_tensor, filename in tqdm(loader):\n        img_tensor = img_tensor.to(DEVICE)\n        \n        # Generate\n        generated = model(img_tensor)\n        \n        # Post-process\n        generated = denormalize(generated).cpu()\n        \n        # Save images\n        # We need to save as jpg\n        for i in range(generated.size(0)):\n            img_np = generated[i].permute(1, 2, 0).numpy()\n            img_np = (img_np * 255).astype(np.uint8)\n            img_pil = Image.fromarray(img_np)\n            \n            # Save to temporary directory\n            save_path = os.path.join(OUTPUT_DIR, filename[i])\n            img_pil.save(save_path)\n            cnt += 1\n\nprint(f\"Generated {cnt} images at {OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T09:44:20.921831Z","iopub.execute_input":"2025-12-16T09:44:20.922793Z","iopub.status.idle":"2025-12-16T10:17:07.279085Z","shell.execute_reply.started":"2025-12-16T09:44:20.922757Z","shell.execute_reply":"2025-12-16T10:17:07.277669Z"}},"outputs":[{"name":"stdout","text":"Loading weights from ../input/monet-gan-weights/G_Photo_epoch_30_2.pth...\nModel initialized with ngf=32, n_blocks=6\nFound 7038 photo images.\nStarting generation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7038/7038 [32:46<00:00,  3.58it/s]","output_type":"stream"},{"name":"stdout","text":"Generated 7038 images at ../tmp/images\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":24},{"id":"ffd399cd","cell_type":"code","source":"\n# --- ZIP RESULTS ---\nimport shutil\n\n# The competition requires a file named 'images.zip' in the working directory\nprint(\"Zipping images...\")\nshutil.make_archive('images', 'zip', OUTPUT_DIR)\nprint(\"✅ Created images.zip\")\n\n# Verify\nif os.path.exists(\"images.zip\"):\n    print(f\"images.zip created successfully. Size: {os.path.getsize('images.zip') / 1024 / 1024:.2f} MB\")\nelse:\n    print(\"❌ Failed to create images.zip\")\n\n# Optional: Clean up temp folder to save space/inodes if needed\n# shutil.rmtree(OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:17:10.790189Z","iopub.execute_input":"2025-12-16T10:17:10.790635Z","iopub.status.idle":"2025-12-16T10:17:14.526063Z","shell.execute_reply.started":"2025-12-16T10:17:10.790487Z","shell.execute_reply":"2025-12-16T10:17:14.524698Z"}},"outputs":[{"name":"stdout","text":"Zipping images...\n✅ Created images.zip\nimages.zip created successfully. Size: 77.71 MB\n","output_type":"stream"}],"execution_count":26}]}