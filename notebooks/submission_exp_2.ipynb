{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Kaggle Paths\n",
    "MONET_PATH = '../input/gan-getting-started/monet_jpg'\n",
    "PHOTO_PATH = '../input/gan-getting-started/photo_jpg'\n",
    "\n",
    "# --- DYNAMIC WEIGHTS SEARCH ---\n",
    "# We search for ANY .pth file in ../input/\n",
    "# This handles cases where the dataset name is different or files are nested.\n",
    "print(\"Searching for weights in ../input/ ...\")\n",
    "found_weights = glob.glob('../input/**/*.pth', recursive=True)\n",
    "\n",
    "WEIGHTS_PATH = None\n",
    "\n",
    "# Prioritize files with 'Monet' and 'epoch' in the name if multiple exist\n",
    "for f in found_weights:\n",
    "    if 'G_Monet' in f and 'epoch' in f:\n",
    "        WEIGHTS_PATH = f\n",
    "        break\n",
    "\n",
    "# Fallback: take the first .pth found if no specific match\n",
    "if WEIGHTS_PATH is None and len(found_weights) > 0:\n",
    "    WEIGHTS_PATH = found_weights[0]\n",
    "\n",
    "if WEIGHTS_PATH:\n",
    "    print(f\"✅ Found weights at: {WEIGHTS_PATH}\")\n",
    "else:\n",
    "    print(\"❌ No weights found! Please check your dataset.\")\n",
    "    # Default fallback for local testing only\n",
    "    WEIGHTS_PATH = 'G_Monet_epoch_30_2.pth'\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = '../tmp/images'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- MODELS (Inlined from src/models/generator/resnet_gan.py) ---\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0),\n",
    "            nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class ResNetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, ngf=32, n_blocks=6, n_downsampling=2):\n",
    "        super(ResNetGenerator, self).__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = ngf\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(n_downsampling):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_blocks):\n",
    "            model += [ResNetBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(n_downsampling):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe744d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- DATASET & TRANSFORMS ---\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.files[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, os.path.basename(img_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "def get_transforms():\n",
    "    # Only resize and normalize for inference\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((256, 256), Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "def denormalize(tensor):\n",
    "    return tensor * 0.5 + 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "# 1. Initialize Model\n",
    "# Experiment 2 used defaults: ngf=32, n_blocks=6\n",
    "model = ResNetGenerator(ngf=32, n_blocks=6).to(DEVICE)\n",
    "\n",
    "# 2. Load Weights\n",
    "# Fallback check for weights in current directory if dataset path fails\n",
    "if not os.path.exists(WEIGHTS_PATH):\n",
    "    if os.path.exists('G_Monet_epoch_30_2.pth'):\n",
    "        WEIGHTS_PATH = 'G_Monet_epoch_30_2.pth'\n",
    "        print(f\"Found weights in current directory: {WEIGHTS_PATH}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Weights not found at {WEIGHTS_PATH}. Please make sure you added the dataset correctly.\")\n",
    "\n",
    "if os.path.exists(WEIGHTS_PATH):\n",
    "    print(f\"Loading weights from {WEIGHTS_PATH}...\")\n",
    "    state_dict = torch.load(WEIGHTS_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(state_dict)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not find weight file at {WEIGHTS_PATH}\")\n",
    "\n",
    "\n",
    "print(f\"Model initialized with ngf=32, n_blocks=6\")\n",
    "# print(model) # Uncomment to debug structure\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 3. Prepare Data\n",
    "import glob\n",
    "photo_files = sorted(glob.glob(os.path.join(PHOTO_PATH, \"*.jpg\")))\n",
    "print(f\"Found {len(photo_files)} photo images.\")\n",
    "\n",
    "# 4. Generate Images\n",
    "dataset = ImageDataset(photo_files, transform=get_transforms())\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Starting generation...\")\n",
    "cnt = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_tensor, filename in tqdm(loader):\n",
    "        img_tensor = img_tensor.to(DEVICE)\n",
    "        \n",
    "        # Generate\n",
    "        generated = model(img_tensor)\n",
    "        \n",
    "        # Post-process\n",
    "        generated = denormalize(generated).cpu()\n",
    "        \n",
    "        # Save images\n",
    "        # We need to save as jpg\n",
    "        for i in range(generated.size(0)):\n",
    "            img_np = generated[i].permute(1, 2, 0).numpy()\n",
    "            img_np = (img_np * 255).astype(np.uint8)\n",
    "            img_pil = Image.fromarray(img_np)\n",
    "            \n",
    "            # Save to temporary directory\n",
    "            save_path = os.path.join(OUTPUT_DIR, filename[i])\n",
    "            img_pil.save(save_path)\n",
    "            cnt += 1\n",
    "\n",
    "print(f\"Generated {cnt} images at {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd399cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- ZIP RESULTS ---\n",
    "import shutil\n",
    "\n",
    "# The competition requires a file named 'images.zip' in the working directory\n",
    "print(\"Zipping images...\")\n",
    "shutil.make_archive('images', 'zip', OUTPUT_DIR)\n",
    "print(\"✅ Created images.zip\")\n",
    "\n",
    "# Verify\n",
    "if os.path.exists(\"images.zip\"):\n",
    "    print(f\"images.zip created successfully. Size: {os.path.getsize('images.zip') / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(\"❌ Failed to create images.zip\")\n",
    "\n",
    "# Optional: Clean up temp folder to save space/inodes if needed\n",
    "# shutil.rmtree(OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
