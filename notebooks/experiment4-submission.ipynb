{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"},{"sourceId":14175205,"sourceType":"datasetVersion","datasetId":9035733}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"15c02ffa","cell_type":"code","source":"\nimport os\nimport shutil\nimport zipfile\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport glob\nimport functools\n\n# --- CONFIGURATION ---\n# Kaggle Paths\nMONET_PATH = '../input/gan-getting-started/monet_jpg'\nPHOTO_PATH = '../input/gan-getting-started/photo_jpg'\n\n# --- DYNAMIC WEIGHTS SEARCH ---\nprint(\"Searching for weights in ../input/ ...\")\nfound_weights = glob.glob('../input/**/*.pth', recursive=True)\n\nWEIGHTS_PATH = None\n\n# Prioritize files for Experiment 4 (look for _4 suffix)\nfor f in found_weights:\n    if 'G_Monet' in f and 'epoch' in f and '_4' in f:\n        WEIGHTS_PATH = f\n        break\n\n# Fallback: take the first .pth found if no specific match\nif WEIGHTS_PATH is None and len(found_weights) > 0:\n    WEIGHTS_PATH = found_weights[0]\n\nif WEIGHTS_PATH:\n    print(f\"✅ Found weights at: {WEIGHTS_PATH}\")\nelse:\n    print(\"❌ No weights found! Please check your dataset.\")\n    WEIGHTS_PATH = 'G_Monet_epoch_30_4.pth' # Default fallback\n\n# Output\nOUTPUT_DIR = '../tmp/images'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:48:03.422359Z","iopub.execute_input":"2025-12-16T10:48:03.423150Z","iopub.status.idle":"2025-12-16T10:48:25.507627Z","shell.execute_reply.started":"2025-12-16T10:48:03.423115Z","shell.execute_reply":"2025-12-16T10:48:25.506749Z"}},"outputs":[{"name":"stdout","text":"Searching for weights in ../input/ ...\n✅ Found weights at: ../input/monet-exp4-checkpoints/G_Photo_epoch_30_4.pth\nUsing device: cpu\n","output_type":"stream"}],"execution_count":1},{"id":"94174838","cell_type":"code","source":"\n# --- MODELS (Inlined from src/models/generator/unet_gan.py) ---\n\nclass UNetGenerator(nn.Module):\n    def __init__(self, input_nc=3, output_nc=3, num_downs=6, ngf=32):\n        super(UNetGenerator, self).__init__()\n        \n        # Construct U-Net structure\n        unet_block = UNetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, innermost=True)  # add the innermost layer\n        \n        # Reduced number of intermediate layers\n        # For num_downs=6, we subtract 5 (outermost + innermost + 3 reduction layers) = 1 intermediate layer\n        for i in range(num_downs - 5):\n            unet_block = UNetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block)\n        \n        # Gradually reduce the number of filters from ngf * 8 to ngf\n        unet_block = UNetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block)\n        unet_block = UNetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block)\n        unet_block = UNetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block)\n        \n        self.model = UNetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True)  # add the outermost layer\n\n    def forward(self, x):\n        return self.model(x)\n\nclass UNetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc, input_nc=None, submodule=None, outermost=False, innermost=False, norm_layer=nn.InstanceNorm2d, use_dropout=False):\n        super(UNetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        if input_nc is None:\n            input_nc = outer_nc\n        \n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:   # add skip connections\n            return torch.cat([x, self.model(x)], 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:49:15.437856Z","iopub.execute_input":"2025-12-16T10:49:15.438320Z","iopub.status.idle":"2025-12-16T10:49:15.453821Z","shell.execute_reply.started":"2025-12-16T10:49:15.438266Z","shell.execute_reply":"2025-12-16T10:49:15.452798Z"}},"outputs":[],"execution_count":2},{"id":"4863a565","cell_type":"code","source":"\n# --- DATASET & TRANSFORMS ---\n\nclass ImageDataset(Dataset):\n    def __init__(self, files, transform=None):\n        self.files = files\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img_path = self.files[index]\n        img = Image.open(img_path).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, os.path.basename(img_path)\n\n    def __len__(self):\n        return len(self.files)\n\ndef get_transforms():\n    # Only resize and normalize for inference (Matches Exp 4 training)\n    return transforms.Compose([\n        transforms.Resize((256, 256), Image.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\ndef denormalize(tensor):\n    return tensor * 0.5 + 0.5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:49:18.614933Z","iopub.execute_input":"2025-12-16T10:49:18.615247Z","iopub.status.idle":"2025-12-16T10:49:18.622246Z","shell.execute_reply.started":"2025-12-16T10:49:18.615224Z","shell.execute_reply":"2025-12-16T10:49:18.621362Z"}},"outputs":[],"execution_count":3},{"id":"715e7e13","cell_type":"code","source":"\n# --- MAIN EXECUTION ---\n\n# 1. Initialize Model\n# Experiment 4 used Lighter U-Net (ngf=16)\nmodel = UNetGenerator(num_downs=6, ngf=16).to(DEVICE)\n\n# 2. Load Weights\nif os.path.exists(WEIGHTS_PATH):\n    print(f\"Loading weights from {WEIGHTS_PATH}...\")\n    state_dict = torch.load(WEIGHTS_PATH, map_location=DEVICE)\n    model.load_state_dict(state_dict)\nelse:\n    raise FileNotFoundError(f\"Could not find weight file at {WEIGHTS_PATH}\")\n\nmodel.eval()\n\n# 3. Prepare Data\nimport glob\nphoto_files = sorted(glob.glob(os.path.join(PHOTO_PATH, \"*.jpg\")))\nprint(f\"Found {len(photo_files)} photo images.\")\n\n# 4. Generate Images\ndataset = ImageDataset(photo_files, transform=get_transforms())\nloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n\nprint(\"Starting generation...\")\ncnt = 0\n\nwith torch.no_grad():\n    for img_tensor, filename in tqdm(loader):\n        img_tensor = img_tensor.to(DEVICE)\n        \n        # Generate\n        generated = model(img_tensor)\n        \n        # Post-process\n        generated = denormalize(generated).cpu()\n        \n        # Save images\n        # We need to save as jpg\n        for i in range(generated.size(0)):\n            img_np = generated[i].permute(1, 2, 0).numpy()\n            img_np = (img_np * 255).astype(np.uint8)\n            img_pil = Image.fromarray(img_np)\n            \n            # Save to temporary directory\n            save_path = os.path.join(OUTPUT_DIR, filename[i])\n            img_pil.save(save_path)\n            cnt += 1\n\nprint(f\"Generated {cnt} images at {OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:49:21.793838Z","iopub.execute_input":"2025-12-16T10:49:21.794157Z","iopub.status.idle":"2025-12-16T10:52:34.852516Z","shell.execute_reply.started":"2025-12-16T10:49:21.794132Z","shell.execute_reply":"2025-12-16T10:52:34.851441Z"}},"outputs":[{"name":"stdout","text":"Loading weights from ../input/monet-exp4-checkpoints/G_Photo_epoch_30_4.pth...\nFound 7038 photo images.\nStarting generation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7038/7038 [03:12<00:00, 36.50it/s]","output_type":"stream"},{"name":"stdout","text":"Generated 7038 images at ../tmp/images\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"id":"a283e199","cell_type":"code","source":"\n# --- ZIP RESULTS ---\nimport shutil\n\n# The competition requires a file named 'images.zip' in the working directory\nprint(\"Zipping images...\")\nshutil.make_archive('images', 'zip', OUTPUT_DIR)\nprint(\"✅ Created images.zip\")\n\n# Verify\nif os.path.exists(\"images.zip\"):\n    print(f\"images.zip created successfully. Size: {os.path.getsize('images.zip') / 1024 / 1024:.2f} MB\")\nelse:\n    print(\"❌ Failed to create images.zip\")\n\n# Optional: Clean up temp folder to save space/inodes if needed\n# shutil.rmtree(OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:52:38.019654Z","iopub.execute_input":"2025-12-16T10:52:38.019941Z","iopub.status.idle":"2025-12-16T10:52:41.214351Z","shell.execute_reply.started":"2025-12-16T10:52:38.019911Z","shell.execute_reply":"2025-12-16T10:52:41.213548Z"}},"outputs":[{"name":"stdout","text":"Zipping images...\n✅ Created images.zip\nimages.zip created successfully. Size: 83.36 MB\n","output_type":"stream"}],"execution_count":6}]}