{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN Experiment 3: U-Net Generator with Enhanced Logging\n",
    "\n",
    "This notebook replicates Experiment 2 but using a **U-Net Generator** architecture instead of ResNet. It also includes enhanced WandB logging (test & train images) and limits image logging to prevent crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab. Skipping Drive mount.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    REPO_PATH = '/content/drive/MyDrive/Kaggle_GANS_I-m-Something-of-a-Painter-Myself_Competition'\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/MonetGAN_Checkpoints_Exp1'\n",
    "    \n",
    "    # Clone if not exists\n",
    "    if not os.path.exists(REPO_PATH):\n",
    "        print(\"Repository not found in Drive. Please clone it first (see experiment2.ipynb).\")\n",
    "    else:\n",
    "        os.chdir(REPO_PATH)\n",
    "        print(f\"Changed directory to {REPO_PATH}\")\n",
    "else:\n",
    "    # Local fallback\n",
    "    REPO_PATH = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    CHECKPOINT_DIR = os.path.join(REPO_PATH, 'checkpoints')\n",
    "    if os.path.exists(REPO_PATH):\n",
    "        os.chdir(REPO_PATH)\n",
    "        print(f\"Changed directory to {REPO_PATH}\")\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Add src to path so we can import modules\n",
    "sys.path.append('src')\n",
    "\n",
    "# Import U-Net Generator instead of ResNet\n",
    "from models.generator.unet_gan import UNetGenerator \n",
    "from models.discriminator.patch_gan import PatchDiscriminator\n",
    "from utils.dataset import ImageDataset, get_transforms\n",
    "from utils.helpers import ReplayBuffer, weights_init_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to WandB\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    wandb_key = userdata.get('WANDB_API_KEY')\n",
    "    wandb.login(key=wandb_key)\n",
    "except Exception:\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "MONET_PATH = 'data/monet_jpg'\n",
    "PHOTO_PATH = 'data/photo_jpg'\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "N_EPOCHS = 30\n",
    "LR = 0.0002\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(\n",
    "    project=\"Monet_GAN_Experiment3_UNet\",\n",
    "    entity=\"konstantine25b-free-university-of-tbilisi-\",\n",
    "    config={\n",
    "        \"epochs\": N_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LR,\n",
    "        \"architecture\": \"CycleGAN-UNet\", # Changed to U-Net\n",
    "        \"dataset\": \"Monet2Photo\",\n",
    "        \"experiment\": \"Experiment 3 (U-Net)\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "monet_files = sorted(glob.glob(os.path.join(MONET_PATH, \"*.*\")))\n",
    "photo_files = sorted(glob.glob(os.path.join(PHOTO_PATH, \"*.*\")))\n",
    "\n",
    "print(f\"Total Monet images: {len(monet_files)}\")\n",
    "print(f\"Total Photo images: {len(photo_files)}\")\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(monet_files)\n",
    "random.shuffle(photo_files)\n",
    "\n",
    "def split_data(files, is_monet=False):\n",
    "    n_val = 1 if is_monet else 50\n",
    "    if is_monet:\n",
    "        val = files[:n_val]\n",
    "        train = files[n_val:]\n",
    "        test = []\n",
    "    else:\n",
    "        n_test = 30\n",
    "        val = files[:n_val]\n",
    "        test = files[n_val:n_val+n_test]\n",
    "        train = files[n_val+n_test:]\n",
    "    return train, val, test\n",
    "\n",
    "monet_train, monet_val, monet_test = split_data(monet_files, is_monet=True)\n",
    "photo_train, photo_val, photo_test = split_data(photo_files, is_monet=False)\n",
    "\n",
    "transforms_ = get_transforms()\n",
    "train_dataset = ImageDataset(monet_train, photo_train, transform=transforms_)\n",
    "val_dataset = ImageDataset(monet_val, photo_val, transform=transforms_)\n",
    "test_dataset = ImageDataset(monet_val, photo_test, transform=transforms_)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    return tensor * 0.5 + 0.5\n",
    "\n",
    "# Visualize Data\n",
    "batch = next(iter(train_loader))\n",
    "real_monet = batch['monet']\n",
    "real_photo = batch['photo']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(denormalize(real_monet[0]).permute(1, 2, 0).cpu().numpy())\n",
    "plt.title(\"Real Monet\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(denormalize(real_photo[0]).permute(1, 2, 0).cpu().numpy())\n",
    "plt.title(\"Real Photo\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization (U-Net)\n",
    "G_Monet = UNetGenerator().to(DEVICE) # Using U-Net\n",
    "G_Photo = UNetGenerator().to(DEVICE) # Using U-Net\n",
    "D_Monet = PatchDiscriminator().to(DEVICE)\n",
    "D_Photo = PatchDiscriminator().to(DEVICE)\n",
    "\n",
    "G_Monet.apply(weights_init_normal)\n",
    "G_Photo.apply(weights_init_normal)\n",
    "D_Monet.apply(weights_init_normal)\n",
    "D_Photo.apply(weights_init_normal)\n",
    "\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_Monet.parameters(), G_Photo.parameters()),\n",
    "    lr=LR, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D_Monet = torch.optim.Adam(D_Monet.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "optimizer_D_Photo = torch.optim.Adam(D_Photo.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=lambda epoch: 1.0 - max(0, epoch - 25) / float(50 - 25 + 1)\n",
    ")\n",
    "lr_scheduler_D_Monet = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_Monet, lr_lambda=lambda epoch: 1.0 - max(0, epoch - 25) / float(50 - 25 + 1)\n",
    ")\n",
    "lr_scheduler_D_Photo = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_Photo, lr_lambda=lambda epoch: 1.0 - max(0, epoch - 25) / float(50 - 25 + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Logic (checking for _3 suffix for experiment 3 if any, or starting fresh)\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    files = glob.glob(os.path.join(checkpoint_dir, \"G_Monet_epoch_*_3.pth\"))\n",
    "    if not files:\n",
    "        return 0\n",
    "    epochs = [int(re.search(r'epoch_(\\d+)_3.pth', f).group(1)) for f in files]\n",
    "    return max(epochs)\n",
    "\n",
    "start_epoch = find_latest_checkpoint(CHECKPOINT_DIR)\n",
    "\n",
    "if start_epoch > 0:\n",
    "    print(f\"Found checkpoint! Resuming from epoch {start_epoch}...\")\n",
    "    G_Monet.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f'G_Monet_epoch_{start_epoch}_3.pth'), map_location=DEVICE))\n",
    "    G_Photo.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f'G_Photo_epoch_{start_epoch}_3.pth'), map_location=DEVICE))\n",
    "    D_Monet.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f'D_Monet_epoch_{start_epoch}_3.pth'), map_location=DEVICE))\n",
    "    D_Photo.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f'D_Photo_epoch_{start_epoch}_3.pth'), map_location=DEVICE))\n",
    "\n",
    "    for _ in range(start_epoch):\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_Monet.step()\n",
    "        lr_scheduler_D_Photo.step()\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Fixed samples for visualization\n",
    "val_batch = next(iter(val_loader))\n",
    "fixed_photo_val = val_batch['photo'].to(DEVICE)\n",
    "fixed_monet_val = val_batch['monet'].to(DEVICE)\n",
    "\n",
    "train_batch = next(iter(train_loader))\n",
    "fixed_photo_train = train_batch['photo'].to(DEVICE)\n",
    "fixed_monet_train = train_batch['monet'].to(DEVICE)\n",
    "\n",
    "fake_monet_buffer = ReplayBuffer()\n",
    "fake_photo_buffer = ReplayBuffer()\n",
    "\n",
    "def show_generated_images(real_p, fake_m, real_m, fake_p, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    images = [real_p, fake_m, real_m, fake_p]\n",
    "    titles = ['Real Photo', 'Generated Monet', 'Real Monet', 'Generated Photo']\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow(denormalize(img[0]).permute(1, 2, 0).cpu().detach().numpy())\n",
    "        plt.title(f\"{titles[i]} {title_suffix}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Starting training from epoch {start_epoch} to {N_EPOCHS}...\")\n",
    "\n",
    "for epoch in range(start_epoch, N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Training ---\n",
    "    G_Monet.train(); G_Photo.train(); D_Monet.train(); D_Photo.train()\n",
    "\n",
    "    epoch_loss_G = 0.0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\")):\n",
    "        real_monet = batch['monet'].to(DEVICE)\n",
    "        real_photo = batch['photo'].to(DEVICE)\n",
    "\n",
    "        # Train Generators\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        loss_id_A = criterion_identity(G_Monet(real_monet), real_monet)\n",
    "        loss_id_B = criterion_identity(G_Photo(real_photo), real_photo)\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        fake_monet = G_Monet(real_photo)\n",
    "        loss_GAN_AB = criterion_GAN(D_Monet(fake_monet), torch.ones_like(D_Monet(fake_monet)))\n",
    "\n",
    "        fake_photo = G_Photo(real_monet)\n",
    "        loss_GAN_BA = criterion_GAN(D_Photo(fake_photo), torch.ones_like(D_Photo(fake_photo)))\n",
    "\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        rec_photo = G_Photo(fake_monet)\n",
    "        loss_cycle_A = criterion_cycle(rec_photo, real_photo)\n",
    "\n",
    "        rec_monet = G_Monet(fake_photo)\n",
    "        loss_cycle_B = criterion_cycle(rec_monet, real_monet)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        loss_G = loss_GAN + (10.0 * loss_cycle) + (5.0 * loss_identity)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        epoch_loss_G += loss_G.item()\n",
    "\n",
    "        # Train Discriminators\n",
    "        optimizer_D_Monet.zero_grad()\n",
    "        loss_real = criterion_GAN(D_Monet(real_monet), torch.ones_like(D_Monet(real_monet)))\n",
    "        fake_monet_ = fake_monet_buffer.push_and_pop(fake_monet)\n",
    "        loss_fake = criterion_GAN(D_Monet(fake_monet_.detach()), torch.zeros_like(D_Monet(fake_monet_)))\n",
    "        loss_D_Monet = (loss_real + loss_fake) / 2\n",
    "        loss_D_Monet.backward()\n",
    "        optimizer_D_Monet.step()\n",
    "\n",
    "        optimizer_D_Photo.zero_grad()\n",
    "        loss_real = criterion_GAN(D_Photo(real_photo), torch.ones_like(D_Photo(real_photo)))\n",
    "        fake_photo_ = fake_photo_buffer.push_and_pop(fake_photo)\n",
    "        loss_fake = criterion_GAN(D_Photo(fake_photo_.detach()), torch.zeros_like(D_Photo(fake_photo_)))\n",
    "        loss_D_Photo = (loss_real + loss_fake) / 2\n",
    "        loss_D_Photo.backward()\n",
    "        optimizer_D_Photo.step()\n",
    "\n",
    "        wandb.log({\n",
    "            \"Loss/G_Total\": loss_G.item(),\n",
    "            \"Loss/G_GAN\": loss_GAN.item(),\n",
    "            \"Loss/G_Cycle\": loss_cycle.item(),\n",
    "            \"Loss/G_Identity\": loss_identity.item(),\n",
    "            \"Loss/D_Monet\": loss_D_Monet.item(),\n",
    "            \"Loss/D_Photo\": loss_D_Photo.item()\n",
    "        })\n",
    "\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_Monet.step()\n",
    "    lr_scheduler_D_Photo.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished. Avg Generator Loss: {epoch_loss_G / len(train_loader):.4f}\")\n",
    "\n",
    "    # Print Discriminator Losses for better visibility\n",
    "    avg_loss_D_Monet = loss_D_Monet.item()\n",
    "    avg_loss_D_Photo = loss_D_Photo.item()\n",
    "    print(f\"  > D_Monet Loss: {avg_loss_D_Monet:.4f} | D_Photo Loss: {avg_loss_D_Photo:.4f}\")\n",
    "    print(f\"  > Cycle Loss: {loss_cycle.item():.4f} | Identity Loss: {loss_identity.item():.4f} | GAN Loss: {loss_GAN.item():.4f}\")\n",
    "\n",
    "    # --- Validation / Visualization ---\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f\"--- Visualizing Epoch {epoch+1} Results ---\")\n",
    "        G_Monet.eval()\n",
    "        G_Photo.eval()\n",
    "        with torch.no_grad():\n",
    "            # Validation Sample\n",
    "            fake_monet_val_vis = G_Monet(fixed_photo_val)\n",
    "            fake_photo_val_vis = G_Photo(fixed_monet_val)\n",
    "            show_generated_images(fixed_photo_val, fake_monet_val_vis, fixed_monet_val, fake_photo_val_vis, \"(Val)\")\n",
    "\n",
    "            # Training Sample\n",
    "            fake_monet_train_vis = G_Monet(fixed_photo_train)\n",
    "            fake_photo_train_vis = G_Photo(fixed_monet_train)\n",
    "            show_generated_images(fixed_photo_train, fake_monet_train_vis, fixed_monet_train, fake_photo_train_vis, \"(Train)\")\n",
    "\n",
    "            # WandB Log Images (Limited to 4 images per epoch: 2 Val, 2 Train)\n",
    "            # Slicing [0:1] to ensure single image is logged\n",
    "            wandb.log({\n",
    "                \"Generated/Val_Photo_to_Monet\": [wandb.Image(denormalize(fake_monet_val_vis[0]).cpu(), caption=\"Val Generated Monet\")],\n",
    "                \"Generated/Val_Monet_to_Photo\": [wandb.Image(denormalize(fake_photo_val_vis[0]).cpu(), caption=\"Val Generated Photo\")],\n",
    "                \"Generated/Train_Photo_to_Monet\": [wandb.Image(denormalize(fake_monet_train_vis[0]).cpu(), caption=\"Train Generated Monet\")],\n",
    "                \"Generated/Train_Monet_to_Photo\": [wandb.Image(denormalize(fake_photo_train_vis[0]).cpu(), caption=\"Train Generated Photo\")]\n",
    "            })\n",
    "\n",
    "    # --- Save Checkpoints with _3 suffix ---\n",
    "    torch.save(G_Monet.state_dict(), os.path.join(CHECKPOINT_DIR, f'G_Monet_epoch_{epoch+1}_3.pth'))\n",
    "    torch.save(G_Photo.state_dict(), os.path.join(CHECKPOINT_DIR, f'G_Photo_epoch_{epoch+1}_3.pth'))\n",
    "    torch.save(D_Monet.state_dict(), os.path.join(CHECKPOINT_DIR, f'D_Monet_epoch_{epoch+1}_3.pth'))\n",
    "    torch.save(D_Photo.state_dict(), os.path.join(CHECKPOINT_DIR, f'D_Photo_epoch_{epoch+1}_3.pth'))\n",
    "    print(f\"Saved checkpoint for epoch {epoch+1} (suffix _3) to {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
