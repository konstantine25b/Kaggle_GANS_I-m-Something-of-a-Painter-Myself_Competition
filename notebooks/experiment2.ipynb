{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25abfab4",
   "metadata": {},
   "source": [
    "\n",
    "# CycleGAN Experiment 2: Full Training with Enhanced Logging\n",
    "\n",
    "This notebook replicates Experiment 1 but with full training (no quick test), enhanced WandB logging (test & train images), and modified checkpoint naming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f071f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cad884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GitHub Configuration & Setup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Please double-check this path matches where you cloned the repo in your Drive\n",
    "REPO_PATH = '/content/drive/MyDrive/Kaggle_GANS_I-m-Something-of-a-Painter-Myself_Competition'\n",
    "\n",
    "try:\n",
    "    # 1. Change to the repository directory\n",
    "    if os.path.exists(REPO_PATH):\n",
    "        %cd \"$REPO_PATH\"\n",
    "        print(f\"Changed directory to {REPO_PATH}\")\n",
    "    else:\n",
    "        print(f\"Warning: Could not find repository at {REPO_PATH}.\")\n",
    "        print(\"Please update 'REPO_PATH' variable to point to your cloned repository folder.\")\n",
    "        # Attempting to list MyDrive to help user find the folder\n",
    "        print(\"Listing folders in MyDrive to help identify path:\")\n",
    "        !ls -d /content/drive/MyDrive/*/\n",
    "\n",
    "    # 2. Configure Git\n",
    "    user_name = \"konstantine25b\"\n",
    "    mail = \"konstantine25b@gmail.com\"\n",
    "\n",
    "    # --- IMPORTANT: PASTE YOUR TOKEN BELOW ---\n",
    "    my_token = \"YOUR_TOKEN_HERE\"\n",
    "\n",
    "    if my_token == \"YOUR_TOKEN_HERE\":\n",
    "        print(\"⚠️ PLEASE UPDATE 'my_token' in the code cell with your actual GitHub token to enable pushing.\")\n",
    "\n",
    "    repo_url = f\"https://{my_token}@github.com/konstantine25b/Kaggle_GANS_I-m-Something-of-a-Painter-Myself_Competition.git\"\n",
    "\n",
    "    !git config --global user.name \"{user_name}\"\n",
    "    !git config --global user.email \"{mail}\"\n",
    "\n",
    "    # 3. Set Remote URL\n",
    "    if os.path.isdir(\".git\") and my_token != \"YOUR_TOKEN_HERE\":\n",
    "        !git remote set-url origin \"{repo_url}\"\n",
    "        print(\"Git configured successfully for pushing.\")\n",
    "    else:\n",
    "        print(\"Skipping remote setup (either not a git repo or token not set).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up GitHub: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80bc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install \"wandb[workspaces]\" -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install requirements\n",
    "!pip install -r requirements.txt\n",
    "!pip install wandb -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758445a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Add src to path so we can import modules\n",
    "sys.path.append('src')\n",
    "\n",
    "from models.generator.resnet_gan import ResNetGenerator\n",
    "from models.discriminator.patch_gan import PatchDiscriminator\n",
    "from utils.dataset import ImageDataset, get_transforms\n",
    "from utils.helpers import ReplayBuffer, weights_init_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Login to WandB\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a132cde",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data Loading and Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "MONET_PATH = 'data/monet_jpg'\n",
    "PHOTO_PATH = 'data/photo_jpg'\n",
    "# Keeping the same directory but we will modify filenames\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/MonetGAN_Checkpoints_Exp1'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "N_EPOCHS = 30\n",
    "LR = 0.0002\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- FULL TRAINING ---\n",
    "QUICK_TEST = False \n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(\n",
    "    project=\"Monet_GAn\",\n",
    "    entity=\"konstantine25b-free-university-of-tbilisi-\",\n",
    "    config={\n",
    "        \"epochs\": N_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LR,\n",
    "        \"architecture\": \"CycleGAN-ResNet\",\n",
    "        \"dataset\": \"Monet2Photo\",\n",
    "        \"quick_test\": QUICK_TEST,\n",
    "        \"experiment\": \"Experiment 2\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8d4db",
   "metadata": {},
   "source": [
    "\n",
    "# WandB Report Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get file paths\n",
    "monet_files = sorted(glob.glob(os.path.join(MONET_PATH, \"*.*\")))\n",
    "photo_files = sorted(glob.glob(os.path.join(PHOTO_PATH, \"*.*\")))\n",
    "\n",
    "# Force full dataset (QUICK_TEST is False)\n",
    "print(f\"Total Monet images: {len(monet_files)}\")\n",
    "print(f\"Total Photo images: {len(photo_files)}\")\n",
    "\n",
    "# Shuffle files\n",
    "random.seed(42)\n",
    "random.shuffle(monet_files)\n",
    "random.shuffle(photo_files)\n",
    "\n",
    "def split_data(files, is_monet=False):\n",
    "    n_val = 1\n",
    "    \n",
    "    if is_monet:\n",
    "        val = files[:n_val]\n",
    "        train = files[n_val:]\n",
    "        test = []\n",
    "    else:\n",
    "        # Full training split\n",
    "        n_test = 30 \n",
    "        val = files[:n_val]\n",
    "        test = files[n_val:n_val+n_test]\n",
    "        train = files[n_val+n_test:]\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "monet_train, monet_val, monet_test = split_data(monet_files, is_monet=True)\n",
    "photo_train, photo_val, photo_test = split_data(photo_files, is_monet=False)\n",
    "\n",
    "print(f\"Monet Splits - Train: {len(monet_train)}, Val: {len(monet_val)}, Test: {len(monet_test)}\")\n",
    "print(f\"Photo Splits - Train: {len(photo_train)}, Val: {len(photo_val)}, Test: {len(photo_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Datasets and DataLoaders\n",
    "transforms_ = get_transforms()\n",
    "\n",
    "train_dataset = ImageDataset(monet_train, photo_train, transform=transforms_)\n",
    "val_dataset = ImageDataset(monet_val, photo_val, transform=transforms_)\n",
    "test_dataset = ImageDataset(monet_val, photo_test, transform=transforms_)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d68a70",
   "metadata": {},
   "source": [
    "## 2. Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b583d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def denormalize(tensor):\n",
    "    return tensor * 0.5 + 0.5\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "real_monet = batch['monet']\n",
    "real_photo = batch['photo']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(denormalize(real_monet[0]).permute(1, 2, 0).cpu().numpy())\n",
    "plt.title(\"Real Monet\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(denormalize(real_photo[0]).permute(1, 2, 0).cpu().numpy())\n",
    "plt.title(\"Real Photo\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c6739",
   "metadata": {},
   "source": [
    "## 3. Model Initialization & Resume Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G_Monet = ResNetGenerator().to(DEVICE)\n",
    "G_Photo = ResNetGenerator().to(DEVICE)\n",
    "D_Monet = PatchDiscriminator().to(DEVICE)\n",
    "D_Photo = PatchDiscriminator().to(DEVICE)\n",
    "\n",
    "G_Monet.apply(weights_init_normal)\n",
    "G_Photo.apply(weights_init_normal)\n",
    "D_Monet.apply(weights_init_normal)\n",
    "D_Photo.apply(weights_init_normal)\n",
    "\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_Monet.parameters(), G_Photo.parameters()),\n",
    "    lr=LR, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D_Monet = torch.optim.Adam(D_Monet.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "optimizer_D_Photo = torch.optim.Adam(D_Photo.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=lambda epoch: 1.0 - max(0, epoch - 25) / float(50 - 25 + 1)\n",
    ")\n",
    "lr_scheduler_D_Monet = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_Monet, lr_lambda=lambda epoch: 1.0 - max(0, epoch - 25) / float(50 - 25 + 1)\n",
    ")\n",
    "lr_scheduler_D_Photo = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_Photo, lr_lambda=lambda epoch: 1.0 - max(0, epoch - 25) / float(50 - 25 + 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- RESUME LOGIC (Modified for _2 suffix) ---\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    # Look for files ending with _2.pth\n",
    "    files = glob.glob(os.path.join(checkpoint_dir, \"G_Monet_epoch_*_2.pth\"))\n",
    "    if not files:\n",
    "        return 0\n",
    "    # Extract epoch number properly considering the _2 suffix\n",
    "    # Format is G_Monet_epoch_{epoch}_2.pth\n",
    "    epochs = [int(re.search(r'epoch_(\\d+)_2.pth', f).group(1)) for f in files]\n",
    "    return max(epochs)\n",
    "\n",
    "start_epoch = find_latest_checkpoint(CHECKPOINT_DIR)\n",
    "\n",
    "if start_epoch > 0:\n",
    "    print(f\"Found checkpoint! Resuming from epoch {start_epoch}...\")\n",
    "    # Load with the _2 suffix\n",
    "    G_Monet.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f'G_Monet_epoch_{start_epoch}_2.pth'), map_location=DEVICE))\n",
    "    G_Photo.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f'G_Photo_epoch_{start_epoch}_2.pth'), map_location=DEVICE))\n",
    "    D_Monet.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f'D_Monet_epoch_{start_epoch}_2.pth'), map_location=DEVICE))\n",
    "    D_Photo.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f'D_Photo_epoch_{start_epoch}_2.pth'), map_location=DEVICE))\n",
    "\n",
    "    for _ in range(start_epoch):\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_Monet.step()\n",
    "        lr_scheduler_D_Photo.step()\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa3b3e",
   "metadata": {},
   "source": [
    "## 4. Training Loop with Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa37b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fixed samples for visualization\n",
    "# Validation sample\n",
    "val_batch = next(iter(val_loader))\n",
    "fixed_photo_val = val_batch['photo'].to(DEVICE)\n",
    "fixed_monet_val = val_batch['monet'].to(DEVICE)\n",
    "\n",
    "# Training sample (New)\n",
    "train_batch = next(iter(train_loader))\n",
    "fixed_photo_train = train_batch['photo'].to(DEVICE)\n",
    "fixed_monet_train = train_batch['monet'].to(DEVICE)\n",
    "\n",
    "fake_monet_buffer = ReplayBuffer()\n",
    "fake_photo_buffer = ReplayBuffer()\n",
    "\n",
    "def show_generated_images(real_p, fake_m, real_m, fake_p, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    images = [real_p, fake_m, real_m, fake_p]\n",
    "    titles = ['Real Photo', 'Generated Monet', 'Real Monet', 'Generated Photo']\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow(denormalize(img[0]).permute(1, 2, 0).cpu().detach().numpy())\n",
    "        plt.title(f\"{titles[i]} {title_suffix}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Starting training from epoch {start_epoch} to {N_EPOCHS}...\")\n",
    "\n",
    "for epoch in range(start_epoch, N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- Training ---\n",
    "    G_Monet.train(); G_Photo.train(); D_Monet.train(); D_Photo.train()\n",
    "    \n",
    "    epoch_loss_G = 0.0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\")):\n",
    "        real_monet = batch['monet'].to(DEVICE)\n",
    "        real_photo = batch['photo'].to(DEVICE)\n",
    "        \n",
    "        # Train Generators\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        loss_id_A = criterion_identity(G_Monet(real_monet), real_monet)\n",
    "        loss_id_B = criterion_identity(G_Photo(real_photo), real_photo)\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "        \n",
    "        fake_monet = G_Monet(real_photo)\n",
    "        loss_GAN_AB = criterion_GAN(D_Monet(fake_monet), torch.ones_like(D_Monet(fake_monet)))\n",
    "        \n",
    "        fake_photo = G_Photo(real_monet)\n",
    "        loss_GAN_BA = criterion_GAN(D_Photo(fake_photo), torch.ones_like(D_Photo(fake_photo)))\n",
    "        \n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "        \n",
    "        rec_photo = G_Photo(fake_monet)\n",
    "        loss_cycle_A = criterion_cycle(rec_photo, real_photo)\n",
    "        \n",
    "        rec_monet = G_Monet(fake_photo)\n",
    "        loss_cycle_B = criterion_cycle(rec_monet, real_monet)\n",
    "        \n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "        \n",
    "        loss_G = loss_GAN + (10.0 * loss_cycle) + (5.0 * loss_identity)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        epoch_loss_G += loss_G.item()\n",
    "        \n",
    "        # Train Discriminators\n",
    "        optimizer_D_Monet.zero_grad()\n",
    "        loss_real = criterion_GAN(D_Monet(real_monet), torch.ones_like(D_Monet(real_monet)))\n",
    "        fake_monet_ = fake_monet_buffer.push_and_pop(fake_monet)\n",
    "        loss_fake = criterion_GAN(D_Monet(fake_monet_.detach()), torch.zeros_like(D_Monet(fake_monet_)))\n",
    "        loss_D_Monet = (loss_real + loss_fake) / 2\n",
    "        loss_D_Monet.backward()\n",
    "        optimizer_D_Monet.step()\n",
    "        \n",
    "        optimizer_D_Photo.zero_grad()\n",
    "        loss_real = criterion_GAN(D_Photo(real_photo), torch.ones_like(D_Photo(real_photo)))\n",
    "        fake_photo_ = fake_photo_buffer.push_and_pop(fake_photo)\n",
    "        loss_fake = criterion_GAN(D_Photo(fake_photo_.detach()), torch.zeros_like(D_Photo(fake_photo_)))\n",
    "        loss_D_Photo = (loss_real + loss_fake) / 2\n",
    "        loss_D_Photo.backward()\n",
    "        optimizer_D_Photo.step()\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Loss/G_Total\": loss_G.item(),\n",
    "            \"Loss/G_GAN\": loss_GAN.item(),\n",
    "            \"Loss/G_Cycle\": loss_cycle.item(),\n",
    "            \"Loss/G_Identity\": loss_identity.item(),\n",
    "            \"Loss/D_Monet\": loss_D_Monet.item(),\n",
    "            \"Loss/D_Photo\": loss_D_Photo.item()\n",
    "        })\n",
    "        \n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_Monet.step()\n",
    "    lr_scheduler_D_Photo.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} finished. Avg Generator Loss: {epoch_loss_G / len(train_loader):.4f}\")\n",
    "    \n",
    "    # --- Validation / Visualization ---\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        G_Monet.eval()\n",
    "        G_Photo.eval()\n",
    "        with torch.no_grad():\n",
    "            # Validation Sample\n",
    "            fake_monet_val_vis = G_Monet(fixed_photo_val)\n",
    "            fake_photo_val_vis = G_Photo(fixed_monet_val)\n",
    "            show_generated_images(fixed_photo_val, fake_monet_val_vis, fixed_monet_val, fake_photo_val_vis, \"(Val)\")\n",
    "            \n",
    "            # Training Sample\n",
    "            fake_monet_train_vis = G_Monet(fixed_photo_train)\n",
    "            fake_photo_train_vis = G_Photo(fixed_monet_train)\n",
    "            show_generated_images(fixed_photo_train, fake_monet_train_vis, fixed_monet_train, fake_photo_train_vis, \"(Train)\")\n",
    "            \n",
    "            # WandB Log Images (Val and Train)\n",
    "            wandb.log({\n",
    "                \"Generated/Val_Photo_to_Monet\": [wandb.Image(denormalize(fake_monet_val_vis[0]).cpu(), caption=\"Val Generated Monet\")],\n",
    "                \"Generated/Val_Monet_to_Photo\": [wandb.Image(denormalize(fake_photo_val_vis[0]).cpu(), caption=\"Val Generated Photo\")],\n",
    "                \"Generated/Train_Photo_to_Monet\": [wandb.Image(denormalize(fake_monet_train_vis[0]).cpu(), caption=\"Train Generated Monet\")],\n",
    "                \"Generated/Train_Monet_to_Photo\": [wandb.Image(denormalize(fake_photo_train_vis[0]).cpu(), caption=\"Train Generated Photo\")]\n",
    "            })\n",
    "\n",
    "    # --- Save Checkpoints with _2 suffix ---\n",
    "    torch.save(G_Monet.state_dict(), os.path.join(CHECKPOINT_DIR, f'G_Monet_epoch_{epoch+1}_2.pth'))\n",
    "    torch.save(G_Photo.state_dict(), os.path.join(CHECKPOINT_DIR, f'G_Photo_epoch_{epoch+1}_2.pth'))\n",
    "    torch.save(D_Monet.state_dict(), os.path.join(CHECKPOINT_DIR, f'D_Monet_epoch_{epoch+1}_2.pth'))\n",
    "    torch.save(D_Photo.state_dict(), os.path.join(CHECKPOINT_DIR, f'D_Photo_epoch_{epoch+1}_2.pth'))\n",
    "    print(f\"Saved checkpoint for epoch {epoch+1} (suffix _2) to {CHECKPOINT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78feb60",
   "metadata": {},
   "source": [
    "## 5. Testing with Enhanced Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60337769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(monet_val) == 0:\n",
    "    print(\"WARNING: Monet Validation set is empty! Using Monet Train set for validation/test fallback.\")\n",
    "    monet_val = monet_train[:1]\n",
    "\n",
    "test_dataset = ImageDataset(monet_val, photo_test, transform=transforms_)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False) # Batch size 1 for visualization\n",
    "\n",
    "G_Monet.eval()\n",
    "\n",
    "# Generate and Save ALL Test Results to WandB\n",
    "print(\"Generating test results...\")\n",
    "test_images_log = []\n",
    "\n",
    "for i, batch in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "    real_photo = batch['photo'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        fake_monet = G_Monet(real_photo)\n",
    "    \n",
    "    # Visualize first 5 locally\n",
    "    if i < 5:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(denormalize(real_photo[0]).permute(1, 2, 0).cpu().numpy())\n",
    "        plt.title(\"Real Photo (Test)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(denormalize(fake_monet[0]).permute(1, 2, 0).cpu().numpy())\n",
    "        plt.title(\"Generated Monet (Test)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Log to WandB list\n",
    "    test_images_log.append(wandb.Image(denormalize(fake_monet[0]).cpu(), caption=f\"Test Image {i+1}\"))\n",
    "\n",
    "# Log all test images at once\n",
    "wandb.log({\"Test/All_Generated_Monet\": test_images_log})\n",
    "print(\"Logged all test results to WandB.\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afb8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- AUTOMATED REPORT GENERATION ---\n",
    "# This cell uses the WandB Workspaces API to create a report dashboard automatically.\n",
    "\n",
    "try:\n",
    "    import wandb_workspaces.reports.v2 as wr\n",
    "\n",
    "    # Configure Report\n",
    "    PROJECT = \"Monet_GAn\"\n",
    "    ENTITY = \"konstantine25b-free-university-of-tbilisi-\"\n",
    "\n",
    "    report = wr.Report(\n",
    "        project=PROJECT,\n",
    "        entity=ENTITY,\n",
    "        title=\"CycleGAN Experiment 2 Results (with Images)\",\n",
    "        description=\"Automated report generated from experiment2.ipynb training run.\"\n",
    "    )\n",
    "\n",
    "    # Define Report Structure\n",
    "    report.blocks = [\n",
    "        wr.H1(\"Experiment 2 Overview\"),\n",
    "        wr.P(\"This report tracks the training progress of Experiment 2 (Full Training).\"),\n",
    "        wr.P(f\"Hyperparameters: Epochs={N_EPOCHS}, Batch Size={BATCH_SIZE}, LR={LR}\"),\n",
    "\n",
    "        wr.H2(\"Training Losses\"),\n",
    "        wr.P(\"Tracking the Generator and Discriminator losses over time.\"),\n",
    "        wr.PanelGrid(\n",
    "            runsets=[wr.Runset(project=PROJECT, entity=ENTITY)],\n",
    "            panels=[\n",
    "                wr.LinePlot(x='Step', y=['Loss/G_Total'], title=\"Total Generator Loss\"),\n",
    "                wr.LinePlot(x='Step', y=['Loss/G_GAN', 'Loss/G_Cycle', 'Loss/G_Identity'], title=\"Generator Component Losses\"),\n",
    "                wr.LinePlot(x='Step', y=['Loss/D_Monet', 'Loss/D_Photo'], title=\"Discriminator Losses\")\n",
    "            ]\n",
    "        ),\n",
    "        \n",
    "        wr.H2(\"Validation Results\"),\n",
    "        wr.P(\"Generated Monet and Photo translations from validation set (Logged every epoch).\"),\n",
    "        wr.PanelGrid(\n",
    "            runsets=[wr.Runset(project=PROJECT, entity=ENTITY)],\n",
    "            panels=[\n",
    "                wr.MediaBrowser(\n",
    "                    media_keys=[\"Generated/Val_Photo_to_Monet\", \"Generated/Val_Monet_to_Photo\"],\n",
    "                    num_columns=2\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "\n",
    "        wr.H2(\"Test Results\"),\n",
    "        wr.P(\"Full test set generation (Logged at the end).\"),\n",
    "        wr.PanelGrid(\n",
    "            runsets=[wr.Runset(project=PROJECT, entity=ENTITY)],\n",
    "            panels=[\n",
    "                wr.MediaBrowser(\n",
    "                    media_keys=[\"Test/All_Generated_Monet\"],\n",
    "                    num_columns=4\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Save Report\n",
    "    report.save()\n",
    "    print(f\"✅ WandB Report generated successfully! View it here: {report.url}\")\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"❌ Report generation failed due to API mismatch: {e}\")\n",
    "    print(\"Try updating wandb: !pip install --upgrade wandb\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Report generation failed: {e}\")\n",
    "    print(\"You can still view all results in the main Dashboard link above.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
