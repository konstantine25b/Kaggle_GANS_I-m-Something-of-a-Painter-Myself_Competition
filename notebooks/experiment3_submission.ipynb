{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import functools\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Kaggle Paths\n",
    "MONET_PATH = '../input/gan-getting-started/monet_jpg'\n",
    "PHOTO_PATH = '../input/gan-getting-started/photo_jpg'\n",
    "\n",
    "# --- DYNAMIC WEIGHTS SEARCH ---\n",
    "print(\"Searching for weights in ../input/ ...\")\n",
    "found_weights = glob.glob('../input/**/*.pth', recursive=True)\n",
    "\n",
    "WEIGHTS_PATH = None\n",
    "\n",
    "# Prioritize files for Experiment 3 (look for _3 suffix)\n",
    "for f in found_weights:\n",
    "    if 'G_Monet' in f and 'epoch' in f and '_3' in f:\n",
    "        WEIGHTS_PATH = f\n",
    "        break\n",
    "\n",
    "# Fallback: take the first .pth found if no specific match\n",
    "if WEIGHTS_PATH is None and len(found_weights) > 0:\n",
    "    WEIGHTS_PATH = found_weights[0]\n",
    "\n",
    "if WEIGHTS_PATH:\n",
    "    print(f\"✅ Found weights at: {WEIGHTS_PATH}\")\n",
    "else:\n",
    "    print(\"❌ No weights found! Please check your dataset.\")\n",
    "    WEIGHTS_PATH = 'G_Monet_epoch_30_3.pth' # Default fallback\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = '../tmp/images'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb58960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- MODELS (Inlined from src/models/generator/unet_gan.py) ---\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, num_downs=6, ngf=32):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        \n",
    "        # Construct U-Net structure\n",
    "        unet_block = UNetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, innermost=True)  # add the innermost layer\n",
    "        \n",
    "        # Reduced number of intermediate layers\n",
    "        # For num_downs=6, we subtract 5 (outermost + innermost + 3 reduction layers) = 1 intermediate layer\n",
    "        for i in range(num_downs - 5):\n",
    "            unet_block = UNetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block)\n",
    "        \n",
    "        # Gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UNetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block)\n",
    "        unet_block = UNetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block)\n",
    "        unet_block = UNetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block)\n",
    "        \n",
    "        self.model = UNetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True)  # add the outermost layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class UNetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None, submodule=None, outermost=False, innermost=False, norm_layer=nn.InstanceNorm2d, use_dropout=False):\n",
    "        super(UNetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        \n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:   # add skip connections\n",
    "            return torch.cat([x, self.model(x)], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5530871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- DATASET & TRANSFORMS ---\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.files[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, os.path.basename(img_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "def get_transforms():\n",
    "    # Only resize and normalize for inference\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((256, 256), Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "def denormalize(tensor):\n",
    "    return tensor * 0.5 + 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "# 1. Initialize Model\n",
    "# Experiment 3 used U-Net with ngf=32, num_downs=6\n",
    "model = UNetGenerator(num_downs=6, ngf=32).to(DEVICE)\n",
    "\n",
    "# 2. Load Weights\n",
    "if os.path.exists(WEIGHTS_PATH):\n",
    "    print(f\"Loading weights from {WEIGHTS_PATH}...\")\n",
    "    state_dict = torch.load(WEIGHTS_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(state_dict)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not find weight file at {WEIGHTS_PATH}\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 3. Prepare Data\n",
    "import glob\n",
    "photo_files = sorted(glob.glob(os.path.join(PHOTO_PATH, \"*.jpg\")))\n",
    "print(f\"Found {len(photo_files)} photo images.\")\n",
    "\n",
    "# 4. Generate Images\n",
    "dataset = ImageDataset(photo_files, transform=get_transforms())\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Starting generation...\")\n",
    "cnt = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_tensor, filename in tqdm(loader):\n",
    "        img_tensor = img_tensor.to(DEVICE)\n",
    "        \n",
    "        # Generate\n",
    "        generated = model(img_tensor)\n",
    "        \n",
    "        # Post-process\n",
    "        generated = denormalize(generated).cpu()\n",
    "        \n",
    "        # Save images\n",
    "        # We need to save as jpg\n",
    "        for i in range(generated.size(0)):\n",
    "            img_np = generated[i].permute(1, 2, 0).numpy()\n",
    "            img_np = (img_np * 255).astype(np.uint8)\n",
    "            img_pil = Image.fromarray(img_np)\n",
    "            \n",
    "            # Save to temporary directory\n",
    "            save_path = os.path.join(OUTPUT_DIR, filename[i])\n",
    "            img_pil.save(save_path)\n",
    "            cnt += 1\n",
    "\n",
    "print(f\"Generated {cnt} images at {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c13900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- ZIP RESULTS ---\n",
    "import shutil\n",
    "\n",
    "# The competition requires a file named 'images.zip' in the working directory\n",
    "print(\"Zipping images...\")\n",
    "shutil.make_archive('images', 'zip', OUTPUT_DIR)\n",
    "print(\"✅ Created images.zip\")\n",
    "\n",
    "# Verify\n",
    "if os.path.exists(\"images.zip\"):\n",
    "    print(f\"images.zip created successfully. Size: {os.path.getsize('images.zip') / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(\"❌ Failed to create images.zip\")\n",
    "\n",
    "# Optional: Clean up temp folder to save space/inodes if needed\n",
    "# shutil.rmtree(OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
